{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Cài đặt thư viện ---\n",
    "!pip install -U transformers datasets accelerate bitsandbytes flash-attn scikit-learn peft trl protobuf huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "#login('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Import thư viện ---\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- Cell 3: Đăng nhập vào Hugging Face Hub ---\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Cell 4: Chuyển dữ liệu txt sang dataset ---\n",
    "DATA_DIR = 'dermnetnz_qa'\n",
    "data = []\n",
    "for cat in os.listdir(DATA_DIR):\n",
    "    cat_dir = os.path.join(DATA_DIR, cat)\n",
    "    for fname in os.listdir(cat_dir):\n",
    "        with open(os.path.join(cat_dir, fname), encoding='utf-8') as f:\n",
    "            qa = {}\n",
    "            for line in f:\n",
    "                if line.startswith('Q:'):\n",
    "                    qa['question'] = line[2:].strip()\n",
    "                elif line.startswith('A:'):\n",
    "                    qa['answer'] = line[2:].strip()\n",
    "                elif line.strip() == '' and qa:\n",
    "                    if 'question' in qa and 'answer' in qa:\n",
    "                        data.append(qa)\n",
    "                    qa = {}\n",
    "            if 'question' in qa and 'answer' in qa:\n",
    "                data.append(qa)\n",
    "\n",
    "print(f\"Tổng số cặp QA: {len(data)}\")\n",
    "\n",
    "# --- Cell 5: Chia tập train/val/test ---\n",
    "test_size = 20\n",
    "val_ratio = 0.1\n",
    "\n",
    "trainval_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "val_size = int(len(trainval_data) * val_ratio)\n",
    "train_data, val_data = train_test_split(trainval_data, test_size=val_size, random_state=42)\n",
    "\n",
    "print(f\"Số cặp train: {len(train_data)}, Số cặp val: {len(val_data)}, Số cặp test: {len(test_data)}\")\n",
    "\n",
    "# --- Cell 6: Tạo dataset ---\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_list([{'question': item['question'], 'answer': item['answer']} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{'question': item['question'], 'answer': item['answer']} for item in val_data])\n",
    "test_dataset = Dataset.from_list([{'question': item['question'], 'answer': item['answer']} for item in test_data])\n",
    "\n",
    "# --- Cell 7: Tải LLaMA-2 7B và lượng tử hóa ---\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Cấu hình lượng tử hóa 4-bit để tiết kiệm VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Tải tokenizer - cần sử dụng trust_remote_code\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Tải mô hình với cấu hình lượng tử hóa\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,  # Sử dụng lượng tử hóa 4-bit\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"  # Thay thế use_flash_attention_2\n",
    ")\n",
    "\n",
    "# --- Cell 8: Thêm LoRA adapter ---\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Cấu hình LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # Target modules dành riêng cho Mixtral\n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",  # Thêm layers đặc trưng cho MoE\n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Chuẩn bị mô hình và thêm LoRA adapter\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Cell 9: Chuẩn bị dữ liệu huấn luyện ---\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Format prompt theo chuẩn Mixtral\n",
    "def generate_prompt(example):\n",
    "    return f\"<s>[INST] {example['question']} [/INST] {example['answer']}</s>\"\n",
    "    # Hoặc format ChatML nếu cần\n",
    "    # return f\"<s><im_start>user\\n{example['question']}<im_end>\\n<im_start>assistant\\n{example['answer']}<im_end></s>\"\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(example):\n",
    "    prompt = generate_prompt(example)\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,  # Giảm kích thước để tiết kiệm VRAM\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # Đặt labels bằng input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Áp dụng tokenize\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# Data collator với padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# --- Cell 10: Huấn luyện mô hình với LoRA ---\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Tính số bước cho mỗi epoch\n",
    "steps_per_epoch = len(tokenized_train_dataset) // (8 * 4)  # batch_size * gradient_accumulation_steps\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình huấn luyện cho Mixtral (giảm batch size do mô hình lớn hơn)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./mixtral-8x7b-lora-dermnetz\",\n",
    "    run_name=\"mixtral-8x7b-lora-finetune\",\n",
    "    per_device_train_batch_size=4,        # Giảm xuống từ 16\n",
    "    per_device_eval_batch_size=4,         # Giảm xuống từ 16\n",
    "    gradient_accumulation_steps=8,        # Tăng lên để bù đắp batch size nhỏ\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=400,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    warmup_steps=steps_per_epoch//3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    # Thêm gradient_checkpointing nếu cần\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    # Không sử dụng tokenizer parameter\n",
    ")\n",
    "\n",
    "# --- Cell 11: Huấn luyện ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 12: Lưu model ---\n",
    "# Chỉ lưu adapter LoRA - không lưu mô hình đầy đủ\n",
    "model.save_pretrained(\"./mixtral-8x7b-lora-dermnetz\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./mixtral-8x7b-lora-dermnetz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In kích thước của adapter LoRA đã lưu\n",
    "!du -sh ./mixtral-8x7b-lora-dermnetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 12: Đánh giá loss trên validation set ---\n",
    "import time\n",
    "import torch\n",
    "\n",
    "print(\"=== Đánh giá loss trên toàn bộ validation set ===\")\n",
    "print(f\"Số lượng mẫu trong validation set: {len(val_dataset)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "eval_results = trainer.evaluate()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Validation Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.4f}\")\n",
    "print(f\"Thời gian đánh giá: {end_time - start_time:.2f} giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Test model với so sánh câu trả lời ---\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load base model và tokenizer (giống như cell inference)\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 2. Load base model và adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "adapter_path = \"./mixtral-8x7b-lora-dermnetz\"  # Đường dẫn đến checkpoint\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# 3. Load ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# 4. Hàm tạo prompt\n",
    "def generate_prompt(question):\n",
    "    return f\"<s>[INST] {question} [/INST]\"\n",
    "\n",
    "# 5. Hàm sinh câu trả lời\n",
    "def generate_answer(question, max_new_tokens=512, temperature=0.7):\n",
    "    prompt = generate_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Xử lý đầu ra dựa trên format Mixtral\n",
    "    response = response.split(\"[/INST]\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# 6. Test với tập test data và tính điểm\n",
    "results = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "# Lấy 10 mẫu từ test_dataset để kiểm tra (hoặc toàn bộ nếu ít hơn 10)\n",
    "num_test_samples = min(len(test_dataset), 20)\n",
    "for i in tqdm(range(num_test_samples)):\n",
    "    sample = test_dataset[i]\n",
    "    question = sample['question']\n",
    "    ground_truth = sample['answer']\n",
    "    \n",
    "    # Sinh câu trả lời từ model\n",
    "    model_answer = generate_answer(question)\n",
    "    \n",
    "    # Tính điểm ROUGE\n",
    "    scores = scorer.score(ground_truth, model_answer)\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    results.append({\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth,\n",
    "        'model_answer': model_answer,\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    })\n",
    "    \n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# 7. In kết quả chi tiết và điểm trung bình\n",
    "print(f\"===== TEST RESULTS (AVERAGE SCORES) =====\")\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_scores):.4f}\")  \n",
    "print(f\"ROUGE-L: {np.mean(rougeL_scores):.4f}\")\n",
    "print(\"\\n===== DETAILED RESULTS =====\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n----- Example {i+1} -----\")\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"Ground truth: {result['ground_truth']}\")\n",
    "    print(f\"Model answer: {result['model_answer']}\")\n",
    "    print(f\"ROUGE-1: {result['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {result['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {result['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
