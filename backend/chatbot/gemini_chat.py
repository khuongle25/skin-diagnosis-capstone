import os
import logging
import google.generativeai as genai

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SkinLesionRAG:
    def __init__(self, num_results=5):
        self.num_results = num_results
    
    def get_context(self, lesion_type):
        """Get context information about a specific skin lesion type"""
        logger.info(f"Searching for information about {lesion_type}")
        from .rag_chain import get_full_lesion_name
        lesion_type = get_full_lesion_name(lesion_type)
        # Search for information about the lesion type
        search_query = f"{lesion_type} skin lesion dermatology"
        search_results = google_search(search_query, self.num_results)
        
        contexts = []
        for result in search_results:
            if "url" in result:
                # Scrape content from the URL
                scraped_content = scrape_website(result["url"])
                if scraped_content and len(scraped_content) > 0:
                    logger.info(f"Added content from {result['url']} to context")
                    contexts.append({
                        "title": result.get("title", ""),
                        "url": result["url"],
                        "content": scraped_content[:2000]  # Gi·ªõi h·∫°n n·ªôi dung ƒë·ªÉ tr√°nh qu√° t·∫£i
                    })
        
        # Compile context information
        return self._compile_context(contexts)
    
    def _compile_context(self, contexts):
        """Compile context information into a single string"""
        if not contexts:
            return "No specific information available about this skin lesion type."
        
        compiled_context = ""
        for i, context in enumerate(contexts):
            compiled_context += f"Source {i+1}: {context['title']} ({context['url']})\n"
            compiled_context += f"{context['content'][:800]}...\n\n"
        
        return compiled_context

class SkinLesionChatbot:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("Google API key is required")
        
        # C·∫•u h√¨nh Gemini API
        genai.configure(api_key=self.api_key)
        
        # S·ª≠ d·ª•ng t√™n model ch√≠nh x√°c
        try:
            self.model = genai.GenerativeModel('gemini-1.5-pro')
            # Use gemini flash 8b instead of gemini-1.5-pro:
            # self.model = genai.GenerativeModel('gemini-1.5-flash-8b')
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o gemini-1.5-pro: {str(e)}. Th·ª≠ v·ªõi gemini-pro...")
            self.model = genai.GenerativeModel('gemini-1.5-flash-8b')
            
        self.rag_system = SkinLesionRAG()
    
    def get_response(self, user_message, lesion_type=None, conversation_history=None):
        """
        Get response from Google Gemini based on user message and lesion type context
        """
        try:
            # Initialize conversation history if None
            if conversation_history is None:
                conversation_history = []
                
            full_lesion_name = lesion_type
            try:
                from .rag_chain import get_full_lesion_name
                full_lesion_name = get_full_lesion_name(lesion_type)
            except Exception:
                pass
            
            # Get context information for the lesion type
            context = ""
            system_prompt = f"""You are a helpful assistant that provides information about skin conditions, 
                particularly about {full_lesion_name if full_lesion_name else lesion_type}.

                The patient has been diagnosed with: **{full_lesion_name if full_lesion_name else lesion_type}**.

                Provide clear, accurate, and helpful information based on medical knowledge.
                Always clarify that you're not a doctor and users should seek professional medical advice.

                IMPORTANT FORMATTING RULES:
                1. Always respond in the SAME LANGUAGE that the user's question is written in.

                2. For BOLD TEXT:
                - Use double asterisks with NO SPACE between the asterisks and text
                - Example: **This is bold text** (correct)
                - NOT: ** This is incorrect ** (has spaces)

                3. For BULLET POINTS:
                - Use a SINGLE asterisk (*) followed by a SINGLE space at the start of the line
                - Add a BLANK LINE between each bullet point
                - Example:
                    * First point
                    
                    * Second point
                    
                    * Third point

                Additional context information:
                {context}
            """
            
            try:
                # Ph∆∞∆°ng ph√°p 1: S·ª≠ d·ª•ng generative_content tr·ª±c ti·∫øp
                prompt = system_prompt + "\n\nUser: " + user_message + "\nAssistant:"
                
                # Thi·∫øt l·∫≠p c√°c tham s·ªë generation
                generation_config = {
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 1024,
                }
                
                safety_settings = [
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    },
                ]
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )
                
                assistant_response = response.text
                
                logger.info(f"Generated response with {len(assistant_response)} characters")
                return assistant_response
                
            except Exception as e:
                logger.error(f"Generate content error: {str(e)}")
                return f"I'm sorry, I encountered an error while generating a response: {str(e)}"
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            return f"I'm sorry, I encountered an error while processing your request: {str(e)}"
        
import requests
from chatbot.rag_chain import SkinLesionRAGChain

class HybridSkinLesionChatbot:
    def __init__(self, api_key, user_id, conversation_id):
        self.api_key = api_key
        self.user_id = user_id
        self.conversation_id = conversation_id
        self.ft_url = "https://e3ce-35-201-128-252.ngrok-free.app/chat"

        genai.configure(api_key=self.api_key)
        try:
            # self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-8b')
            self.gemini_model = genai.GenerativeModel('gemini-1.5-pro')
        except Exception:
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-8b')

    def get_ft_answer(self, question_vi, lesion_type, history=None):
        from .rag_chain import get_full_lesion_name
        full_lesion_name = get_full_lesion_name(lesion_type)
        
        # GEMINI T·ªîNG H·ª¢P MEMORY + D·ªäCH
        if history and len(history) > 0:
            # C√≥ l·ªãch s·ª≠ - Gemini t·ªïng h·ª£p memory + d·ªãch
            history_text = ""
            for msg in history[-5:]:  # 5 l∆∞·ª£t g·∫ßn nh·∫•t
                role = "Ng∆∞·ªùi d√πng" if msg["role"] == "human" else "AI"
                history_text += f"{role}: {msg['content']}\n"
            
            memory_prompt = f"""
            B·∫°n l√† chuy√™n gia y khoa da li·ªÖu AI. NHI·ªÜM V·ª§ L·∫¶N L∆Ø·ª¢T:

            1. PH√ÇN T√çCH NG·ªÆ C·∫¢NH: D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i v√† c√¢u h·ªèi m·ªõi
            2. T·ªîNG H·ª¢P TH√îNG TIN: K·∫øt h·ª£p th√¥ng tin ch·∫©n ƒëo√°n v·ªõi ng·ªØ c·∫£nh h·ªôi tho·∫°i
            3. D·ªäCH SANG TI·∫æNG ANH: T·∫°o c√¢u h·ªèi ti·∫øng Anh ho√†n ch·ªânh cho m√¥ h√¨nh chuy√™n s√¢u

            **TH√îNG TIN CH·∫®N ƒêO√ÅN:**
            - Lo·∫°i t·ªïn th∆∞∆°ng: {full_lesion_name} 

            **L·ªäCH S·ª¨ H·ªòI THO·∫†I:**
            {history_text}

            **C√ÇU H·ªéI M·ªöI (ti·∫øng Vi·ªát):**
            "{question_vi}"

            **Y√äU C·∫¶U ƒê·∫¶U RA:**
            T·∫°o c√¢u h·ªèi ti·∫øng Anh ho√†n ch·ªânh, bao g·ªìm:
            - Th√¥ng tin ch·∫©n ƒëo√°n, ph·∫£i ƒë·∫£m b·∫£o ch·ªß th·ªÉ cu·ªôc tr√≤ chuy·ªán c√≥ ch·ªß th·ªÉ ch√≠nh l√† lo·∫°i t·ªïn th∆∞∆°ng {full_lesion_name} 
            - Ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i (n·∫øu c√≥ th√¥ng tin li√™n quan)
            - C√¢u h·ªèi ƒë∆∞·ª£c d·ªãch ch√≠nh x√°c ng·ªØ nghƒ©a y khoa da li·ªÖu

            CH·ªà tr·∫£ v·ªÅ c√¢u h·ªèi ti·∫øng Anh cu·ªëi c√πng, KH√îNG gi·∫£i th√≠ch:
            """
            
            try:
                response = self.gemini_model.generate_content(memory_prompt)
                # print(f"Gemini memory synthesis response: {response.text}")
                logger.info(f"Gemini memory synthesis response: {response.text}")
                question_with_context = response.text.strip()
            except Exception as e:
                logger.error(f"Gemini memory synthesis error: {str(e)}")
                # Fallback: d·ªãch ƒë∆°n gi·∫£n
                question_en = self.gemini_translate(question_vi)
                question_with_context = f"[Patient diagnosed with: {full_lesion_name} ({lesion_type})] {question_en}"
        else:
            # Kh√¥ng c√≥ l·ªãch s·ª≠ - ch·ªâ d·ªãch ƒë∆°n gi·∫£n
            question_en = self.gemini_translate(question_vi)
            question_with_context = f"[Patient diagnosed with: {full_lesion_name} ({lesion_type})] {question_en}"
        
        # G·ª≠i ƒë·∫øn fine-tuned model
        try:
            resp = requests.post(self.ft_url, json={
                "question": question_with_context,
                "lesion_type": lesion_type,
                "lesion_full_name": full_lesion_name
            })
            return resp.json().get("answer", "")
        except Exception as e:
            logger.error(f"Fine-tuned model error: {str(e)}")
            return f"Fine-tuned model unavailable: {str(e)}"

    def get_rag_answer(self, question_vi, lesion_type, enhance_knowledge=False):
        rag_chain = SkinLesionRAGChain(
            api_key=self.api_key,
            user_id=self.user_id,
            conversation_id=self.conversation_id
        )
        # N·∫øu vectorstore ch∆∞a t·ªìn t·∫°i, kh·ªüi t·∫°o tr∆∞·ªõc (blocking)
        if not rag_chain.vectorstore:
            initialized = rag_chain.initialize_knowledge(lesion_type)
            if not initialized or not rag_chain.vectorstore:
                # N·∫øu scrape l·ªói ho·∫∑c vectorstore v·∫´n ch∆∞a c√≥, tr·∫£ v·ªÅ l·ªói
                return "Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu web do l·ªói kh·ªüi t·∫°o ki·∫øn th·ª©c."
        # ƒê·∫øn ƒë√¢y ch·∫Øc ch·∫Øn ƒë√£ scrape xong v√† c√≥ vectorstore
        return rag_chain.get_response(
            message=question_vi,
            lesion_type=lesion_type,
            enhance_knowledge=enhance_knowledge
        )

    def gemini_translate(self, text, direction="vi_to_en"):
        """
        D·ªãch thu·∫≠t chuy√™n nghi·ªáp
        direction: "vi_to_en" ho·∫∑c "en_to_vi"
        """
        if direction == "vi_to_en":
            prompt = (
                "B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t y khoa chuy√™n ng√†nh da li·ªÖu. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:\n"
                "- D·ªãch ch√≠nh x√°c c√¢u h·ªèi t·ª´ ti·∫øng Vi·ªát sang ti·∫øng Anh.\n"
                "- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ y khoa da li·ªÖu, kh√¥ng d·ªãch sai ho·∫∑c l√†m m·∫•t √Ω nghƒ©a chuy√™n ng√†nh.\n"
                "- Kh√¥ng d·ªãch theo nghƒ©a ƒëen, m√† ph·∫£i hi·ªÉu ƒë√∫ng ng·ªØ c·∫£nh ng∆∞·ªùi b·ªánh.\n"
                f'ƒê·∫ßu v√†o ti·∫øng Vi·ªát:\n"{text}"\n'
                "ƒê·∫ßu ra mong mu·ªën (ti·∫øng Anh, ch√≠nh x√°c ng·ªØ nghƒ©a y khoa da li·ªÖu):"
            )
        else:  # en_to_vi
            prompt = (
                "B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t y khoa chuy√™n ng√†nh da li·ªÖu. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:\n"
                "- D·ªãch ch√≠nh x√°c c√¢u tr·∫£ l·ªùi t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát.\n"
                "- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ y khoa da li·ªÖu, kh√¥ng d·ªãch sai ho·∫∑c l√†m m·∫•t √Ω nghƒ©a chuy√™n ng√†nh.\n"
                "- D·ªãch t·ª± nhi√™n, d·ªÖ hi·ªÉu cho b·ªánh nh√¢n Vi·ªát Nam.\n"
                "- CH·ªà tr·∫£ v·ªÅ b·∫£n d·ªãch ti·∫øng Vi·ªát, KH√îNG gi·∫£i th√≠ch.\n"
                f'ƒê·∫ßu v√†o ti·∫øng Anh:\n"{text}"\n'
                "ƒê·∫ßu ra mong mu·ªën (ti·∫øng Vi·ªát, t·ª± nhi√™n, d·ªÖ hi·ªÉu):"
            )
        
        try:
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Translation error ({direction}): {str(e)}")
            return text
        
    def evaluate_and_fix_answer(self, question, ft_answer_en, lesion_type):
            """
            ƒê√°nh gi√° m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh fine-tune.
            N·∫øu c√¢u tr·∫£ l·ªùi l·∫°c ƒë·ªÅ ho·∫∑c kh√¥ng tr·∫£ l·ªùi tr·ª±c ti·∫øp, Gemini s·∫Ω t·∫°o l·∫°i c√¢u tr·∫£ l·ªùi.
            """
            eval_prompt = f"""
            
            B·∫°n l√† m·ªôt chuy√™n gia y khoa da li·ªÖu AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° t√≠nh li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi.
            - N·∫øu c√¢u tr·∫£ l·ªùi d√π kh√¥ng ho√†n h·∫£o nh∆∞ng v·∫´n li√™n quan v√† c√≥ t·ªìn t·∫°i d√π ch·ªâ m·ªôt √Ω tr·∫£ l·ªùi ƒë√∫ng v√†o tr·ªçng t√¢m c√¢u h·ªèi, h√£y tr·∫£ l·ªùi: "OK".
            - N·∫øu c√¢u tr·∫£ l·ªùi l·∫°c ƒë·ªÅ, chung chung ho·∫∑c ch·ªâ b·∫£o ng∆∞·ªùi d√πng ki·ªÉm tra ph·∫ßn kh√°c thay v√¨ tr·∫£ l·ªùi, h√£y tr·∫£ l·ªùi: "FIX".
            KH√îNG gi·∫£i th√≠ch g√¨ th√™m.
            
            Question: "{question}"

            Answer: "{ft_answer_en}"
            """
            try:
                eval_result = self.gemini_model.generate_content(eval_prompt).text.strip()
            except Exception as e:
                logger.error(f"Gemini evaluation error: {str(e)}")
                return ft_answer_en, False  

            if eval_result.strip().upper() == "OK":
                return ft_answer_en, False  

            fix_prompt = f"""
            You are a medical assistant. The patient has been diagnosed with: {lesion_type}.
            Please answer the following question in a helpful, direct, and informative way for the patient, based on medical knowledge. Do not refer the user to other sections or resources. Do not mention you are an AI.

            Question: "{question}"
            """
            try:
                fixed_answer = self.gemini_model.generate_content(fix_prompt).text.strip()
                # print("=== [Gemini Intervene] ===")
                # print(f"Original FT answer: {ft_answer_en}")
                # print(f"Gemini fixed answer: {fixed_answer}")
                # print("==========================")
                logger.info(f"Gemini fixed answer: {fixed_answer}")
                logger.info(f"Original FT answer: {ft_answer_en}")
                logger.info("======================")    
                return fixed_answer, True
            except Exception as e:
                logger.error(f"Gemini fix error: {str(e)}")
                return ft_answer_en, False

    def synthesize_final_answer(self, question_vi, ft_answer_vi, rag_answer, lesion_type):
        """
        T·ªïng h·ª£p hai ngu·ªìn tr·∫£ l·ªùi (m√¥ h√¨nh chuy√™n s√¢u + RAG) th√†nh m·ªôt ƒëo·∫°n ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, ∆∞u ti√™n th√¥ng tin y khoa ch√≠nh x√°c.
        Tr·∫£ v·ªÅ ti·∫øng Vi·ªát, kh√¥ng l·∫∑p l·∫°i nguy√™n vƒÉn, kh√¥ng gi·∫£i th√≠ch th√™m.
        """
        from .rag_chain import get_full_lesion_name
        full_lesion_name = get_full_lesion_name(lesion_type)
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω y khoa AI chuy√™n da li·ªÖu. H√£y t·ªïng h·ª£p th√¥ng tin t·ª´ hai ngu·ªìn tr·∫£ l·ªùi d∆∞·ªõi ƒë√¢y th√†nh m·ªôt ƒëo·∫°n ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, ∆∞u ti√™n th√¥ng tin y khoa ch√≠nh x√°c, d√†nh cho b·ªánh nh√¢n Vi·ªát Nam.
        - Kh√¥ng l·∫∑p l·∫°i nguy√™n vƒÉn t·ª´ng ƒëo·∫°n. Tuy·ªát ƒë·ªëi kh√¥ng t·ª± √Ω th√™m th√¥ng tin ngo√†i hai ngu·ªìn.
        - N·∫øu c√≥ m√¢u thu·∫´n, h√£y ch·ªâ r√µ m√¢u thu·∫´n ƒë√≥ gi·ªØa hai ngu·ªìn tr·∫£ l·ªùi.
        - Ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n t·ªïng h·ª£p, kh√¥ng gi·∫£i th√≠ch th√™m.
        - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.

        **Ch·∫©n ƒëo√°n:** {full_lesion_name} ({lesion_type})
        **C√¢u h·ªèi:** {question_vi}

        **Tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh chuy√™n s√¢u:**
        {ft_answer_vi}

        **Tr·∫£ l·ªùi t·ª´ web y khoa (RAG):**
        {rag_answer if rag_answer else 'Kh√¥ng c√≥ th√¥ng tin b·ªï sung t·ª´ web.'}

        ===
        ƒêo·∫°n t·ªïng h·ª£p d√†nh cho b·ªánh nh√¢n:
        """
        try:
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Gemini synthesis error: {str(e)}")
            return "Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin do l·ªói h·ªá th·ªëng. Vui l√≤ng tham kh·∫£o c√°c ph·∫ßn th√¥ng tin b√™n d∆∞·ªõi."

    def chat(self, question_vi, lesion_type, enhance_knowledge=False, history=None):
            from .rag_chain import get_full_lesion_name
            full_lesion_name = get_full_lesion_name(lesion_type)

            # 1. Fine-tuned model (Gemini t·ªïng h·ª£p memory + d·ªãch) - TRUY·ªÄN question_vi v√† history
            ft_answer_en = self.get_ft_answer(question_vi, lesion_type, history=history)

            # ƒê√°nh gi√° v√† can thi·ªáp n·∫øu c·∫ßn
            ft_answer_en_checked, intervened = self.evaluate_and_fix_answer(
                question_vi, ft_answer_en, full_lesion_name
            )

            # 2. D·ªãch ft_answer t·ª´ Anh ‚Üí Vi·ªát
            ft_answer_vi = ""
            if ft_answer_en_checked and ft_answer_en_checked.strip():
                ft_answer_vi = self.gemini_translate(ft_answer_en_checked, direction="en_to_vi")

            # 3. RAG answer (n·∫øu enhance_knowledge=True)
            rag_answer = ""
            if enhance_knowledge:
                rag_answer = self.get_rag_answer(question_vi, lesion_type, enhance_knowledge=True)

            # 4. SYNTHESIS: Ch·ªâ t·ªïng h·ª£p n·∫øu c√≥ enhance_knowledge v√† rag_answer
            diagnosis_info = f"ü©∫ **Ch·∫©n ƒëo√°n**: {full_lesion_name} ({lesion_type})\n\n"
            final_answer = ""
            if enhance_knowledge and rag_answer:
                synthesis = self.synthesize_final_answer(question_vi, ft_answer_vi, rag_answer, lesion_type)
                if synthesis:
                    final_answer += f"**ü§ñ Th√¥ng tin t·ªïng h·ª£p:**\n{synthesis}\n\n"
            # 5. SIMPLE CONCATENATION (gi·ªØ nguy√™n)
            final_answer += diagnosis_info
            final_answer += f"**‚ùì C√¢u h·ªèi**: {question_vi}\n\n"
            if ft_answer_vi:
                final_answer += f"**üìö Th√¥ng tin t·ª´ m√¥ h√¨nh chuy√™n s√¢u:**\n{ft_answer_vi}\n\n"
            if enhance_knowledge:
                if rag_answer:
                    final_answer += f"**üåê Th√¥ng tin b·ªï sung t·ª´ web y khoa:**\n{rag_answer}\n\n"
                else:
                    final_answer += f"**üåê Th√¥ng tin t·ª´ web:**\n‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y th√¥ng tin t·ª´ web.\n\n"
            else:
                final_answer += f"**üåê Th√¥ng tin t·ª´ web:**\nüí° *B·∫≠t 'T√¨m ki·∫øm Internet' ƒë·ªÉ c√≥ th√™m th√¥ng tin*\n\n"
            final_answer += "‚ö†Ô∏è **L∆∞u √Ω**: Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o. Vui l√≤ng tham kh·∫£o b√°c sƒ© chuy√™n khoa."

            return final_answer, ft_answer_en_checked, rag_answer
